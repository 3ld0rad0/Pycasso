{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qUrE6AUsYdJ"
      },
      "source": [
        "# SET UP DDPM_Conditional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc3Fs4eXsOXr"
      },
      "outputs": [],
      "source": [
        "!pip3 install --upgrade pip\n",
        "!pip3 install einops\n",
        "!pip3 install transformers\n",
        "!pip3 install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5kEeyR6tODn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torchvision\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch import optim\n",
        "from einops.layers.torch import Rearrange\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbiXog6lS-0_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCHHuMF-tOKH"
      },
      "outputs": [],
      "source": [
        "# connect to Google Drive and unzip the folder for training;\n",
        "!unzip \"./drive/MyDrive/{dataset_folder}.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLWFUFrmtOK7"
      },
      "outputs": [],
      "source": [
        "# setting the parameters\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "image_size = 64\n",
        "num_classes = 21\n",
        "dataset_path = \"./{dataset_folder}/\"\n",
        "lr = 3e-4\n",
        "time_embed_dim = 256\n",
        "path_drive = \"./drive/My Drive/{model_folder}/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting reproducibility\n",
        "SEED = 3\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqW4BgaHtORo"
      },
      "outputs": [],
      "source": [
        "# this flag if true loads the model from {path_drive};TRAIN ONLY;\n",
        "EXIST_MODEL = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmoQd9vutOSk"
      },
      "outputs": [],
      "source": [
        "# getting device;\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\\t\" + (f\"{torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"CPU\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbVdz6qOuiSP"
      },
      "source": [
        "# Utils for DDPM_Conditional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLDR8LDrunG-"
      },
      "outputs": [],
      "source": [
        "def plot_images(images):\n",
        "    plt.figure(figsize=(32, 32))\n",
        "    plt.imshow(torch.cat([\n",
        "        torch.cat([i for i in images.cpu()], dim=-1),\n",
        "    ], dim=-2).permute(1, 2, 0).cpu())\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def save_images(images, path, **kwargs):\n",
        "    grid = torchvision.utils.make_grid(images, **kwargs)\n",
        "    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n",
        "    im = Image.fromarray(ndarr)\n",
        "    im.save(path)\n",
        "\n",
        "\n",
        "# get the data and transforms the images, then pass the dataset to the DataLoader\n",
        "def get_data():\n",
        "    transforms = torchvision.transforms.Compose([\n",
        "        # data augmentation\n",
        "        torchvision.transforms.Resize(80),  # args.image_size + 1/4 *args.image_size\n",
        "        torchvision.transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    dataset = torchvision.datasets.ImageFolder(dataset_path, transform=transforms)\n",
        "    #print(dataset.class_to_idx)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "# return a similar label to the label passed as argument, from the list of class_labels know in the dataset train folder;\n",
        "# in this way is possible to condition the model to generate image from a text/label not present in the class_labels of the train dataset;\n",
        "# CLASS_LABEL_TRAIN contain the label for every conditional label used in the training, in my case these are the 21 labels of dataset_3 using in the training of the model;\n",
        "\n",
        "# if you have to test the model use this:\n",
        "CLASS_LABEL_TRAIN = ['00000', '00001', '00010', '00011', '00100', '00110', '01000', '01010', '01011', '01100', '01110', '10000', '10001', '10010', '10011', '10100', '10110', '11000', '11010', '11100', '11110']\n",
        "\n",
        "# else use this for train the model with another dataset and sets of labels:\n",
        "#CLASS_LABEL_TRAIN = []\n",
        "\n",
        "def get_similar_class(label):\n",
        "\n",
        "    if len(CLASS_LABEL_TRAIN) != 0:\n",
        "        dirs = CLASS_LABEL_TRAIN\n",
        "\n",
        "    else:\n",
        "        dirs = os.listdir(dataset_path)\n",
        "        dirs.sort()\n",
        "\n",
        "    similar_list = []\n",
        "    for i in range(len(dirs)):\n",
        "        if label == dirs[i]:\n",
        "            similar_list.append(i)\n",
        "\n",
        "    dirs_copy = dirs[:]\n",
        "    while len(similar_list) == 0 and len(label) > 1:\n",
        "        truncated_list = []\n",
        "        for s in dirs_copy:\n",
        "            s_truncated = s[:-1]\n",
        "            truncated_list.append(s_truncated)\n",
        "        label = label[:-1]\n",
        "        for i in range(len(truncated_list)):\n",
        "            if label == truncated_list[i]:\n",
        "                similar_list.append(i)\n",
        "        dirs_copy = truncated_list\n",
        "    j = similar_list[0]\n",
        "\n",
        "    return j\n",
        "\n",
        "\n",
        "\n",
        "# the Data Loader load the label of any class in the format 0 to num_classes, for this reason i have to rebind any label to the index, for example label \"00111\" to class index \"3\";\n",
        "# next label to text convert text from the label, in this way i have a description text for any class;TRAIN ONLY;\n",
        "def get_label_index(labels):\n",
        "    text = []\n",
        "    descriptions = []\n",
        "    dirs = os.listdir(dataset_path)\n",
        "    dirs.sort()\n",
        "    labels = labels.detach().cpu().numpy()\n",
        "\n",
        "    for i in labels:\n",
        "        text.append(dirs[i])\n",
        "\n",
        "    for j in text:\n",
        "        descriptions.append(label_to_text(j))\n",
        "\n",
        "\n",
        "    return descriptions\n",
        "\n",
        "# get the text for n sample label; this method is used to generate image from the text;\n",
        "def get_text_for_sample(label,n):\n",
        "    descriptions = []\n",
        "    for i in range(n):\n",
        "        text = label_to_text(label)\n",
        "        descriptions.append(text)\n",
        "    return descriptions\n",
        "\n",
        "FEATURE_LABEL = ['frangetta','occhiali','barba','sorridente','giovane']\n",
        "\n",
        "# transform the label to the respective text, to pass later to text encoder;\n",
        "def label_to_text(label):\n",
        "    text = ''\n",
        "\n",
        "    for i in range(len(FEATURE_LABEL)):\n",
        "        if label[i] == '1':\n",
        "            text += FEATURE_LABEL[i]+','\n",
        "\n",
        "    text+='viso'+','\n",
        "    s = text.split(',')[::-1]\n",
        "    l = []\n",
        "    for i in s:\n",
        "        # appending reversed words to l;\n",
        "        l.append(i)\n",
        "        # printing reverse words;\n",
        "    l.remove('')\n",
        "    text = \" \".join(l)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "# plot the loss function for the training;\n",
        "\n",
        "#def plot_loss(losses):\n",
        "#    plt.plot(losses)\n",
        "#    plt.title('Loss Graph')\n",
        "#    plt.xlabel('Epoch')\n",
        "#    plt.ylabel('Loss')\n",
        "#    plt.show()\n",
        "#    plt.savefig(\"Loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vDSl2drymGD"
      },
      "source": [
        "# T5 Text Encoder\n",
        "Load a pretrained Text encoder model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqaL_dFUykp7"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n",
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "\n",
        "MAX_LENGTH = 256\n",
        "\n",
        "DEFAULT_T5_NAME = 't5_small'\n",
        "\n",
        "# Variants: https://huggingface.co/docs/transformers/model_doc/t5v1.1. 1.1 versions must be finetuned.\n",
        "T5_VERSIONS = {\n",
        "    't5_small': {'tokenizer': None, 'model': None, 'handle': 't5-small', 'dim': 512, 'size': .24},\n",
        "    't5_base': {'tokenizer': None, 'model': None, 'handle': 't5-base', 'dim': 768, 'size': .890},\n",
        "    't5_large': {'tokenizer': None, 'model': None, 'handle': 't5-large', 'dim': 1024, 'size': 2.75},\n",
        "    't5_3b': {'tokenizer': None, 'model': None, 'handle': 't5-3b', 'dim': 1024, 'size': 10.6},\n",
        "    't5_11b': {'tokenizer': None, 'model': None, 'handle': 't5-11b', 'dim': 1024, 'size': 42.1},\n",
        "    'small1.1': {'tokenizer': None, 'model': None, 'handle': 'google/t5-v1_1-small', 'dim': 512, 'size': .3},\n",
        "    'base1.1': {'tokenizer': None, 'model': None, 'handle': 'google/t5-v1_1-base', 'dim': 768, 'size': .99},\n",
        "    'large1.1': {'tokenizer': None, 'model': None, 'handle': 'google/t5-v1_1-large', 'dim': 1024, 'size': 3.13},\n",
        "    'xl1.1': {'tokenizer': None, 'model': None, 'handle': 'google/t5-v1_1-xl', 'dim': 2048, 'size': 11.4},\n",
        "    'xxl1.1': {'tokenizer': None, 'model': None, 'handle': 'google/t5-v1_1-xxl', 'dim': 4096, 'size': 44.5},\n",
        "}\n",
        "\n",
        "# Fast tokenizers: https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
        "def _check_downloads(name):\n",
        "    if T5_VERSIONS[name]['tokenizer'] is None:\n",
        "        T5_VERSIONS[name]['tokenizer'] = T5Tokenizer.from_pretrained(T5_VERSIONS[name]['handle'])\n",
        "    if T5_VERSIONS[name]['model'] is None:\n",
        "        T5_VERSIONS[name]['model'] = T5EncoderModel.from_pretrained(T5_VERSIONS[name]['handle'])\n",
        "\n",
        "\n",
        "def t5_encode_text(text, name: str = 't5_small', max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Encodes a sequence of text with a T5 text encoder.\n",
        "    :param text: List of text to encode.\n",
        "    :param name: Name of T5 model to use. Options are:\n",
        "        - :code:`'t5_small'` (~0.24 GB, 512 encoding dim),\n",
        "        - :code:`'t5_base'` (~0.89 GB, 768 encoding dim),\n",
        "        - :code:`'t5_large'` (~2.75 GB, 1024 encoding dim),\n",
        "        - :code:`'t5_3b'` (~10.6 GB, 1024 encoding dim),\n",
        "        - :code:`'t5_11b'` (~42.1 GB, 1024 encoding dim),\n",
        "    :return: Returns encodings and attention mask. Element **[i,j,k]** of the final encoding corresponds to the **k**-th\n",
        "        encoding component of the **j**-th token in the **i**-th input list element.\n",
        "    \"\"\"\n",
        "    _check_downloads(name)\n",
        "    tokenizer = T5_VERSIONS[name]['tokenizer']\n",
        "    model = T5_VERSIONS[name]['model']\n",
        "\n",
        "    # Move to cuda is available\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        model = model.to(device)\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    # Tokenize text\n",
        "    tokenized = tokenizer.batch_encode_plus(\n",
        "        text,\n",
        "        padding='longest',\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",  # Returns torch.tensor instead of python integers\n",
        "    )\n",
        "\n",
        "    input_ids = tokenized.input_ids.to(device)\n",
        "    attention_mask = tokenized.attention_mask.to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Don't need gradient - T5 frozen during Imagen training\n",
        "    with torch.no_grad():\n",
        "        t5_output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        final_encoding = t5_output.last_hidden_state.detach()\n",
        "\n",
        "    # Wherever the encoding is masked, make equal to zero\n",
        "    final_encoding = final_encoding.masked_fill(~rearrange(attention_mask, '... -> ... 1').bool(), 0.)\n",
        "\n",
        "    return final_encoding, attention_mask.bool()\n",
        "\n",
        "\n",
        "def get_encoded_dim(name: str) -> int:\n",
        "    \"\"\"\n",
        "    Gets the encoding dimensionality of a given T5 encoder.\n",
        "    \"\"\"\n",
        "    return T5_VERSIONS[name]['dim']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIlI4YFhzSxU"
      },
      "source": [
        "# Unet Architecture\n",
        "256 time embedding dimension;\n",
        "4 levels depth;\n",
        "4 cross attention;\n",
        "512 bottleneck;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMiftiE5zW6r"
      },
      "outputs": [],
      "source": [
        "# perform the exponential moving average optimization;\n",
        "class EMA:\n",
        "    def __init__(self, beta):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.step = 0\n",
        "\n",
        "    def update_model_average(self, ma_model, current_model):\n",
        "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
        "            old_weight, up_weight = ma_params.data, current_params.data\n",
        "            ma_params.data = self.update_average(old_weight, up_weight)\n",
        "\n",
        "    def update_average(self, old, new):\n",
        "        if old is None:\n",
        "            return new\n",
        "        return old * self.beta + (1 - self.beta) * new\n",
        "\n",
        "    def step_ema(self, ema_model, model, step_start_ema=2000):\n",
        "        if self.step < step_start_ema:\n",
        "            self.reset_parameters(ema_model, model)\n",
        "            self.step += 1\n",
        "            return\n",
        "        self.update_model_average(ema_model, model)\n",
        "        self.step += 1\n",
        "\n",
        "    def reset_parameters(self, ema_model, model):\n",
        "        ema_model.load_state_dict(model.state_dict())\n",
        "\n",
        "\n",
        "# Q = image; K = V = text embedding;\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, channels, size):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.size = size\n",
        "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
        "        self.ln = nn.LayerNorm([channels])\n",
        "        self.ff_self = nn.Sequential(\n",
        "            nn.LayerNorm([channels]),\n",
        "            nn.Linear(channels, channels),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(channels, channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, te):\n",
        "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
        "        x_ln = self.ln(x)\n",
        "        attention_value, _ = self.mha(x_ln, te, te)\n",
        "        attention_value = attention_value + x\n",
        "        attention_value = self.ff_self(attention_value) + attention_value\n",
        "\n",
        "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
        "        super().__init__()\n",
        "        self.residual = residual\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.GroupNorm(1, mid_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.GroupNorm(1, out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.residual:\n",
        "            return F.gelu(x + self.double_conv(x))\n",
        "        else:\n",
        "            return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim=time_embed_dim):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, in_channels, residual=True),\n",
        "            DoubleConv(in_channels, out_channels),\n",
        "        )\n",
        "\n",
        "        self.emb_layer = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_dim,\n",
        "                out_channels\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        x = self.maxpool_conv(x)\n",
        "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
        "        return x + emb\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim=time_embed_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ct = nn.ConvTranspose2d(in_channels,out_channels,kernel_size=3,padding = 1)\n",
        "        self.up = nn.Upsample(scale_factor= 2, mode=\"bilinear\", align_corners=True)\n",
        "        self.conv = nn.Sequential(\n",
        "            DoubleConv(in_channels, in_channels, residual=True),\n",
        "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
        "        )\n",
        "\n",
        "        self.emb_layer = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_dim,\n",
        "                out_channels\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_x, t):\n",
        "        x = self.ct(x)\n",
        "        x = self.up(x)\n",
        "\n",
        "        x = torch.cat([skip_x, x], dim=1)\n",
        "        x = self.conv(x)\n",
        "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
        "        return x + emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "346t_4qNzbdS"
      },
      "outputs": [],
      "source": [
        "class UNet_conditional(nn.Module):\n",
        "    def __init__(self, c_in=3, c_out=3, time_dim=time_embed_dim, num_classes=None, device=device):\n",
        "            super().__init__()\n",
        "            self.device = device\n",
        "            self.time_dim = time_dim\n",
        "\n",
        "            # text encoder return text embeddings of 512, this Linear layers rearrange the dimension to use the text embeddings in the cross attention layers;\n",
        "            self.to_time_tokens_128 = nn.Sequential(\n",
        "                nn.Linear(self.time_dim, 128 *2),\n",
        "                Rearrange('b (r d) -> b r d', r=2)\n",
        "            )\n",
        "            self.to_time_tokens_256 = nn.Sequential(\n",
        "                nn.Linear(self.time_dim, 256 *2),\n",
        "                Rearrange('b (r d) -> b r d', r=2)\n",
        "            )\n",
        "\n",
        "            self.te_to_dim_128 = nn.Linear(512, 128)\n",
        "            self.te_to_dim_256 = nn.Linear(512, 256)\n",
        "\n",
        "            self.norm_128 = nn.LayerNorm(128)\n",
        "            self.norm_256 = nn.LayerNorm(256)\n",
        "\n",
        "            self.inc = DoubleConv(c_in, 32)\n",
        "            self.down1 = Down(32, 64)\n",
        "            self.down2 = Down(64, 128)\n",
        "            self.ca2 = CrossAttention(128,16)\n",
        "            self.down3 = Down(128, 256)\n",
        "            self.ca3 = CrossAttention(256, 8)\n",
        "            self.down4 = Down(256,512)\n",
        "            self.bot1 = DoubleConv(512, 512)\n",
        "            self.up1 = Up(512, 256)\n",
        "            self.ca4 = CrossAttention(256, 8)\n",
        "            self.up2 = Up(256, 128)\n",
        "            self.ca5 = CrossAttention(128,16)\n",
        "            self.up3 = Up(128, 64)\n",
        "            self.up4 = Up(64, 32)\n",
        "            self.outc = nn.Conv2d(32, c_out, kernel_size=1)\n",
        "\n",
        "            if num_classes is not None:\n",
        "                self.label_emb = nn.Embedding(num_classes, time_dim)\n",
        "\n",
        "    def pos_encoding(self, t, channels):\n",
        "        inv_freq = 1.0 / (\n",
        "            10000\n",
        "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
        "        )\n",
        "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
        "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
        "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
        "        return pos_enc\n",
        "\n",
        "    # generate a toke of the time embeddings to cancatenate it to the text embedding later\n",
        "    def generate_t_tokens(self,t,cond_dim):\n",
        "        if cond_dim == 256:\n",
        "            time_tokens = self.to_time_tokens_256(t)\n",
        "        elif cond_dim == 128:\n",
        "            time_tokens = self.to_time_tokens_128(t)\n",
        "        elif cond_dim == 64:\n",
        "            time_tokens = self.to_time_tokens_64(t)\n",
        "\n",
        "        return time_tokens\n",
        "\n",
        "    # concat time embedding and text embeddings\n",
        "    def concat_time_text(self,te,t,out_dim):\n",
        "        tt = self.generate_t_tokens(t,out_dim)\n",
        "        te_t = torch.cat((te,tt),dim=-2)\n",
        "        if out_dim == 256:\n",
        "            te_t = self.norm_256(te_t)\n",
        "        elif out_dim == 128:\n",
        "            te_t = self.norm_128(te_t)\n",
        "        elif out_dim == 64:\n",
        "            te_t = self.norm_64(te_t)\n",
        "        return te_t\n",
        "\n",
        "    def reduce_dim(self,te,dim_out):\n",
        "        if dim_out == 256:\n",
        "            te = self.te_to_dim_256(te)\n",
        "        elif dim_out == 128:\n",
        "            te = self.te_to_dim_128(te)\n",
        "        elif dim_out == 64:\n",
        "            te = self.te_to_dim_64(te)\n",
        "        return te\n",
        "\n",
        "    def forward(self, x, t, y,te):\n",
        "        t = t.unsqueeze(-1).type(torch.float)\n",
        "        t = self.pos_encoding(t, self.time_dim)\n",
        "        if y is not None:\n",
        "            t += self.label_emb(y)\n",
        "\n",
        "\n",
        "        te_256 = self.reduce_dim(te,256)\n",
        "        te_128 = self.reduce_dim(te,128)\n",
        "\n",
        "        tte_256 = self.concat_time_text(te_256,t,256)\n",
        "        tte_128 = self.concat_time_text(te_128,t,128)\n",
        "\n",
        "\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1, t)\n",
        "        x3 = self.down2(x2, t)\n",
        "        x3 = self.ca2(x3,tte_128)\n",
        "        x4 = self.down3(x3, t)\n",
        "        x4 = self.ca3(x4,tte_256)\n",
        "        x5 = self.down4(x4,t)\n",
        "        x6 = self.bot1(x5)\n",
        "        x = self.up1(x6,x4, t)\n",
        "        x = self.ca4(x,tte_256)\n",
        "        x = self.up2(x, x3, t)\n",
        "        x = self.ca5(x,tte_128)\n",
        "        x = self.up3(x, x2, t)\n",
        "        x = self.up4(x, x1, t)\n",
        "        output = self.outc(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9dggcxH0AgT"
      },
      "source": [
        "# Define the model : train and sample\n",
        "\n",
        "Every 20 epochs there is a checkpoint;\n",
        "N.B. The files will be saved on drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRdYen590Iwi"
      },
      "outputs": [],
      "source": [
        "class Diffusion:\n",
        "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=64, device=device):\n",
        "        self.noise_steps = noise_steps\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "\n",
        "        self.beta = self.prepare_noise_schedule().to(device)\n",
        "        self.alpha = 1. - self.beta\n",
        "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    # cosine schedule;\n",
        "    def betas_cosine(self,num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n",
        "        betas = []\n",
        "        for i in range(num_diffusion_timesteps):\n",
        "            t1 = i / num_diffusion_timesteps\n",
        "            t2 = (i + 1) / num_diffusion_timesteps\n",
        "            betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
        "        betas = torch.tensor(betas)\n",
        "        return betas\n",
        "\n",
        "    # if cosine is False return le linear noising schedule;\n",
        "    def prepare_noise_schedule(self,cosine = False):\n",
        "        if cosine:\n",
        "            beta = self.betas_cosine(self.noise_steps,lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,)\n",
        "            return beta\n",
        "        else :\n",
        "            return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
        "\n",
        "    # noising the image at timestep t;\n",
        "    def noise_images(self, x, t):\n",
        "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
        "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
        "        Ɛ = torch.randn_like(x)\n",
        "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n",
        "\n",
        "    def sample_timesteps(self, n):\n",
        "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
        "\n",
        "    # sampling;\n",
        "    def sample(self, model, n, labels, cfg_scale=3):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
        "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
        "                t = (torch.ones(n) * i).long().to(self.device)\n",
        "                te_labels = get_label_index(labels)\n",
        "                te = t5_encode_text(te_labels)\n",
        "                te = te[0].to(self.device)\n",
        "                predicted_noise = model(x, t, labels,te)\n",
        "                if cfg_scale > 0:\n",
        "                    uncond_predicted_noise = model(x, t, None,te)\n",
        "                    predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, cfg_scale)\n",
        "                alpha = self.alpha[t][:, None, None, None]\n",
        "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
        "                beta = self.beta[t][:, None, None, None]\n",
        "                if i > 1:\n",
        "                    noise = torch.randn_like(x)\n",
        "                else:\n",
        "                    noise = torch.zeros_like(x)\n",
        "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
        "        model.train()\n",
        "        x = (x.clamp(-1, 1) + 1) / 2\n",
        "        x = (x * 255).type(torch.uint8)\n",
        "        return x\n",
        "\n",
        "    # text to image generation, this module in used for Generate the final images;\n",
        "    def generate_image_from_label(self,model,n,label):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
        "            y_index = get_similar_class(label)\n",
        "            y = torch.Tensor([y_index] * n).long().to(self.device)\n",
        "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
        "                t = (torch.ones(n) * i).long().to(self.device)\n",
        "                te_labels = get_text_for_sample(label,n)\n",
        "                te = t5_encode_text(te_labels)\n",
        "                te = te[0].to(self.device)\n",
        "                predicted_noise = model(x, t, y,te)\n",
        "\n",
        "                alpha = self.alpha[t][:, None, None, None]\n",
        "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
        "                beta = self.beta[t][:, None, None, None]\n",
        "                if i > 1:\n",
        "                    noise = torch.randn_like(x)\n",
        "                else:\n",
        "                    noise = torch.zeros_like(x)\n",
        "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
        "\n",
        "        x = (x.clamp(-1, 1) + 1) / 2\n",
        "        x = (x * 255).type(torch.uint8)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkdzIAzN1ues"
      },
      "source": [
        "# Start the train loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpVw5QPx72Jn"
      },
      "outputs": [],
      "source": [
        "# train the model and every 20 epochs sample and plot 21 image for every class_label;\n",
        "def train():\n",
        "    dataloader = get_data()\n",
        "    model = UNet_conditional(num_classes=num_classes).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    mse = nn.MSELoss()\n",
        "    diffusion = Diffusion(img_size=image_size, device=device)\n",
        "    l = len(dataloader)\n",
        "    ema = EMA(0.995)\n",
        "    ema_model = copy.deepcopy(model).eval().requires_grad_(False)\n",
        "\n",
        "    if EXIST_MODEL :\n",
        "        print(\"Recover model...\")\n",
        "        model.load_state_dict(torch.load(os.path.join(path_drive+\"models\",  f\"ckpt.pt\")))\n",
        "        model.eval()\n",
        "        ema_model.load_state_dict(torch.load(os.path.join(path_drive+\"models\",  f\"ema_ckpt.pt\")))\n",
        "        ema_model.eval()\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(path_drive+\"models\",  f\"optim.pt\")))\n",
        "\n",
        "    losses = []\n",
        "    best_loss = float(\"inf\")\n",
        "    for epoch in range(epochs+1):\n",
        "        print(\"Starting epoch :\"+str(epoch))\n",
        "        epoch_loss = 0.0\n",
        "        pbar = tqdm(dataloader)\n",
        "        for i, (images, labels) in enumerate(pbar):\n",
        "            te_labels = get_label_index(labels)\n",
        "            te = t5_encode_text(te_labels)\n",
        "            te = te[0].to(device)\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
        "            x_t, noise = diffusion.noise_images(images, t)\n",
        "            if np.random.random() < 0.1:\n",
        "                labels = None\n",
        "            predicted_noise = model(x_t, t, labels,te)\n",
        "            loss = mse(noise, predicted_noise)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            ema.step_ema(ema_model, model)\n",
        "\n",
        "            pbar.set_postfix(MSE=loss.item())\n",
        "            epoch_loss += loss.item() * len(images) / len(dataloader.dataset)\n",
        "        \n",
        "        losses.append(epoch_loss)\n",
        "        log_string = f\"Loss at epoch {epoch}: {epoch_loss:.3f}\"\n",
        "        print(log_string)\n",
        "\n",
        "        # storing the model;\n",
        "        if best_loss > epoch_loss:\n",
        "            best_loss = epoch_loss\n",
        "            torch.save(model.state_dict(), os.path.join(path_drive+\"models\",  f\"ckpt.pt\"))\n",
        "            torch.save(ema_model.state_dict(), os.path.join(path_drive+\"models\",  f\"ema_ckpt.pt\"))\n",
        "            torch.save(optimizer.state_dict(), os.path.join(path_drive+\"models\",  f\"optim.pt\"))\n",
        "            print(\" --> Best model ever (stored)\")\n",
        "\n",
        "        if epoch!= 0 and epoch % 20 == 0:\n",
        "            #plot_loss(losses)\n",
        "            labels = torch.arange(21).long().to(device)\n",
        "            #sampled_images = diffusion.sample(model, n=len(labels), labels=labels)\n",
        "            ema_sampled_images = diffusion.sample(ema_model, n=len(labels), labels=labels)\n",
        "            plot_images(ema_sampled_images)\n",
        "            #save_images(sampled_images, os.path.join(path_drive+\"results\",  f\"{epoch}.jpg\"))\n",
        "            save_images(ema_sampled_images, os.path.join(path_drive+\"results\",  f\"{epoch}_ema.jpg\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc5zEheV10wh"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k56k3iDL2EuK"
      },
      "source": [
        "# Generate 500 new images\n",
        "This module load the model selected by {path_model} and then generate image following the \"test.txt\" labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp1YE3Ll2FQQ"
      },
      "outputs": [],
      "source": [
        "path_model = \"./drive/My Drive/{model_folder}/\"\n",
        "path_test_generate = \"./drive/My Drive/{test_folder}/\"\n",
        "path_test_text = \"./drive/My Drive/test.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNGpFpEf2FVQ"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "\n",
        "def parse(s):\n",
        "    s = s.strip()\n",
        "    return s\n",
        "def separate(s):\n",
        "    p = s.split(';')\n",
        "    return p\n",
        "\n",
        "def prepare_model():\n",
        "    device = \"cuda\"\n",
        "    model = UNet_conditional(num_classes=21).to(device)\n",
        "    ckpt = torch.load(os.path.join(path_model+\"models\",  f\"ema_ckpt.pt\"))\n",
        "    model.load_state_dict(ckpt)\n",
        "    diffusion = Diffusion(img_size=64, device=device)\n",
        "\n",
        "    return diffusion,model\n",
        "\n",
        "def generate(name,label,diffusion,model,iter = 10,n = 1):\n",
        "    s = 'ABCDEFGHIJ'\n",
        "\n",
        "    for i in range (iter):\n",
        "        x = diffusion.generate_image_from_label(model, n, label)\n",
        "        name_iter =name + '_' + s[i]\n",
        "        save_images(x, os.path.join(path_test_generate, f\"{name_iter}.jpg\"))\n",
        "\n",
        "    print(\"Generate images for label \"+ label + \" name \"+name)\n",
        "\n",
        "\n",
        "def start_generate_new_image():\n",
        "    with io.open(path_test_text,\"r\") as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "    parsed_data = []\n",
        "    for i in data:\n",
        "        parsed_data.append(parse(i))\n",
        "\n",
        "    diffusion,model = prepare_model()\n",
        "\n",
        "    for x in parsed_data:\n",
        "        s = separate(x)\n",
        "        file_name = \"\"+s[0]\n",
        "        file_e = \"\"+s[1]\n",
        "        generate(file_name,file_e,diffusion,model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_lvo-DZ2M7A"
      },
      "outputs": [],
      "source": [
        "start_generate_new_image()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6vDSl2drymGD",
        "zIlI4YFhzSxU",
        "m9dggcxH0AgT",
        "LkdzIAzN1ues",
        "k56k3iDL2EuK"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
